{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdae052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE\n",
    "# First load the worksheet dependencies.\n",
    "# Here is the activation function and its derivative.\n",
    "sigma = lambda z : 1 / (1 + np.exp(-z))\n",
    "d_sigma = lambda z : np.cosh(z/2)**(-2) / 4\n",
    "\n",
    "# This function initialises the network with it's structure, it also resets any training already done.\n",
    "def reset_network (n1 = 5, n2 = 4, n3 = 3, random=np.random) :\n",
    "    global W1, W2, W3, W4, b1, b2, b3, b4\n",
    "    W1 = random.randn(n1, 1) / 2\n",
    "    W2 = random.randn(n2, n1) / 2\n",
    "    W3 = random.randn(n3, n2) / 2\n",
    "    W4 = random.randn(2, n3) / 2\n",
    "\n",
    "    b1 = random.randn(n1, 1) / 2\n",
    "    b2 = random.randn(n2, 1) / 2\n",
    "    b3 = random.randn(n3, 1) / 2\n",
    "    b4 = random.randn(2, 1) / 2\n",
    "\n",
    "# This function feeds forward each activation to the next layer. It returns all weighted sums and activations.\n",
    "def network_function(a0) :\n",
    "    z1 = W1 @ a0 + b1\n",
    "    a1 = sigma(z1)\n",
    "\n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigma(z2)\n",
    "\n",
    "    z3 = W3 @ a2 + b3\n",
    "    a3 = sigma(z3)\n",
    "\n",
    "    z4 = W4 @ a3 + b4\n",
    "    a4 = sigma(z4)\n",
    "\n",
    "    return a0, z1, a1, z2, a2, z3, a3, z4, a4\n",
    "\n",
    "# This is the cost function of a neural network with respect to a training set.\n",
    "def cost(x, y) :\n",
    "    return np.linalg.norm(network_function(x)[-1] - y)**2 / x.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6decda34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION\n",
    "\n",
    "# Jacobian for the third layer weights. There is no need to edit this function.\n",
    "def J_W4 (x, y) :\n",
    "    # First get all the activations and weighted sums at each layer of the network.\n",
    "    a0, z1, a1, z2, a2, z3, a3, z4, a4 = network_function(x)\n",
    "    # We'll use the variable J to store parts of our result as we go along, updating it in each line.\n",
    "    # Firstly, we calculate dC/da4, using the expressions above.\n",
    "    J = 2 * (a4 - y)\n",
    "    # Next multiply the result we've calculated by the derivative of sigma, evaluated at z3.\n",
    "    J = J * d_sigma(z4)\n",
    "    # Then we take the dot product (along the axis that holds the training examples) with the final partial derivative,\n",
    "    # i.e. dz3/dW3 = a2\n",
    "    # and divide by the number of training examples, for the average over all training examples.\n",
    "    J = J @ a3.T / x.size\n",
    "    # Finally return the result out of the function.\n",
    "    return J\n",
    "\n",
    "def J_b4 (x, y) :\n",
    "    # As last time, we'll first set up the activations.\n",
    "    a0, z1, a1, z2, a2, z3, a3, z4, a4 = network_function(x)\n",
    "    # Next you should implement the first two partial derivatives of the Jacobian.\n",
    "    # ===COPY TWO LINES FROM THE PREVIOUS FUNCTION TO SET UP THE FIRST TWO JACOBIAN TERMS===\n",
    "    J = J = 2 * (a4 - y)\n",
    "    J = J * d_sigma(z4)\n",
    "    # For the final line, we don't need to multiply by dz3/db3, because that is multiplying by 1.\n",
    "    # We still need to sum over all training examples however.\n",
    "    # There is no need to edit this line.\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J\n",
    "\n",
    "# Jacobian for the third layer weights. There is no need to edit this function.\n",
    "def J_W3 (x, y) :\n",
    "    # First get all the activations and weighted sums at each layer of the network.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    # We'll use the variable J to store parts of our result as we go along, updating it in each line.\n",
    "    # Firstly, we calculate dC/da3, using the expressions above.\n",
    "    J = \n",
    "    # Next multiply the result we've calculated by the derivative of sigma, evaluated at z3.\n",
    "    J = \n",
    "    # Then we take the dot product (along the axis that holds the training examples) with the final partial derivative,\n",
    "    # i.e. dz3/dW3 = a2\n",
    "    # and divide by the number of training examples, for the average over all training examples.\n",
    "    J = \n",
    "    # Finally return the result out of the function.\n",
    "    return J\n",
    "\n",
    "# In this function, you will implement the jacobian for the bias.\n",
    "# As you will see from the partial derivatives, only the last partial derivative is different.\n",
    "# The first two partial derivatives are the same as previously.\n",
    "# ===YOU SHOULD EDIT THIS FUNCTION===\n",
    "def J_b3 (x, y) :\n",
    "    # As last time, we'll first set up the activations.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    # Next you should implement the first two partial derivatives of the Jacobian.\n",
    "    # ===COPY TWO LINES FROM THE PREVIOUS FUNCTION TO SET UP THE FIRST TWO JACOBIAN TERMS===\n",
    "    J = \n",
    "    J = \n",
    "    J = \n",
    "    J = \n",
    "    # For the final line, we don't need to multiply by dz3/db3, because that is multiplying by 1.\n",
    "    # We still need to sum over all training examples however.\n",
    "    # There is no need to edit this line.\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "138c79aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION\n",
    "\n",
    "# Compare this function to J_W3 to see how it changes.\n",
    "# There is no need to edit this function.\n",
    "def J_W2 (x, y) :\n",
    "    #The first two lines are identical to in J_W3.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)    \n",
    "    J = 2 * (a3 - y)\n",
    "    # the next two lines implement da3/da2, first σ' and then W3.\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    # then the final lines are the same as in J_W3 but with the layer number bumped down.\n",
    "    J = J * d_sigma(z2)\n",
    "    J = J @ a1.T / x.size\n",
    "    return J\n",
    "\n",
    "# As previously, fill in all the incomplete lines.\n",
    "# ===YOU SHOULD EDIT THIS FUNCTION===\n",
    "def J_b2 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = \n",
    "    J = \n",
    "    J = (J.T @ W3).T\n",
    "    J = J * d_sigma(z2)\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d09de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION\n",
    "\n",
    "# Fill in all incomplete lines.\n",
    "# ===YOU SHOULD EDIT THIS FUNCTION===\n",
    "def J_W1 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = \n",
    "    J = \n",
    "    J = (J.T @ W3).T\n",
    "    J = J * d_sigma(z2)\n",
    "    J = (J.T @ W2).T\n",
    "    J = J * d_sigma(z1)\n",
    "    J = J @ a0.T / x.size\n",
    "    return J\n",
    "\n",
    "# Fill in all incomplete lines.\n",
    "# ===YOU SHOULD EDIT THIS FUNCTION===\n",
    "def J_b1 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = \n",
    "    J = \n",
    "    J = (J.T @ W3).T\n",
    "    J = J * d_sigma(z2)\n",
    "    J = (J.T @ W2).T\n",
    "    J = J * d_sigma(z1)\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4664525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DLduongcong (N = 500)∶\n",
    "    x = np.arange(0,1,1/N)\n",
    "    y = np.array([\n",
    "      16*np.sin(2*np.pi*x)**3,\n",
    "      13*np.cos(2*np.pi*x) - 5*np.cos(2*2*np.pi*x) - 2*np.cos(3*2*np.pi*x)- np.cos(4*2*np.pi*x)\n",
    "    ]\n",
    "    ) / 20\n",
    "    y = (y+1)/2\n",
    "    x = np.reshape(x,(1,N))\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe01fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = DLduongcong()\n",
    "reset_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3028b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training (x, y, iterations=10000, aggression=3.5, noise=1) :\n",
    "    global W1, W2, W3, W4, b1, b2, b3, b4\n",
    "    fig,ax = plt.subplots(figsize=(8, 8), dpi= 80)\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    xx = np.arange(0,1.01,0.01)\n",
    "    yy = np.arange(0,1.01,0.01)\n",
    "    X, Y = np.meshgrid(xx, yy)\n",
    "    Z = ((X-0.5)**2 + (Y-1)**2)**(1/2) / (1.25)**(1/2)\n",
    "    im = ax.imshow(Z, vmin=0, vmax=1, extent=[0, 1, 1, 0], cmap=blueMap)\n",
    "\n",
    "    ax.plot(y[0],y[1], lw=1.5, color=green);\n",
    "\n",
    "    while iterations>=0 :\n",
    "        j_W1 = J_W1(x, y) * (1 + np.random.randn() * noise)\n",
    "        j_W2 = J_W2(x, y) * (1 + np.random.randn() * noise)\n",
    "        j_W3 = J_W3(x, y) * (1 + np.random.randn() * noise)\n",
    "        j_W4 = J_W4(x, y) * (1 + np.random.randn() * noise)\n",
    "        j_b1 = J_b1(x, y) * (1 + np.random.randn() * noise)\n",
    "        j_b2 = J_b2(x, y) * (1 + np.random.randn() * noise)\n",
    "        j_b3 = J_b3(x, y) * (1 + np.random.randn() * noise)\n",
    "        j_b4 = J_b4(x, y) * (1 + np.random.randn() * noise)\n",
    "\n",
    "        W1 = W1 - j_W1 * aggression\n",
    "        W2 = W2 - j_W2 * aggression\n",
    "        W3 = W3 - j_W3 * aggression\n",
    "        W4 = W4 - j_W4 * aggression\n",
    "        b1 = b1 - j_b1 * aggression\n",
    "        b2 = b2 - j_b2 * aggression\n",
    "        b3 = b3 - j_b3 * aggression\n",
    "        b4 = b4 - j_b4 * aggression\n",
    "\n",
    "        if (iterations%100==0) :\n",
    "            nf = network_function(x)[-1]\n",
    "            ax.plot(nf[0],nf[1], lw=2, color=magentaTrans);\n",
    "        iterations -= 1\n",
    "\n",
    "    nf = network_function(x)[-1]\n",
    "    ax.plot(nf[0],nf[1], lw=2.5, color=orange);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f6755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(x, y, iterations=50000, aggression=7, noise=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
